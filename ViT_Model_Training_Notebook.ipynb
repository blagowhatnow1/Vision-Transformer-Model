{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4r9GdD1U5WQf",
    "outputId": "d153cfe6-093f-4a88-8459-e705daaa91bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7acc2193cdf0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import ToTensor, Compose, Normalize, Resize\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import StepLR  # or ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eZFj-e-5XUlz"
   },
   "outputs": [],
   "source": [
    "#Set Random Seed\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-IMScJ2qXs18"
   },
   "source": [
    "Define ViT Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7_mKIfeA5a2s"
   },
   "outputs": [],
   "source": [
    "def patchify(images, n_patches):\n",
    "    n, c, h, w = images.shape\n",
    "    assert h == w, \"Patchify method is implemented for square images only\"\n",
    "    patch_size = h // n_patches\n",
    "    patches = torch.zeros(n, n_patches**2, c * patch_size * patch_size)\n",
    "\n",
    "    for idx, image in enumerate(images):\n",
    "        for i in range(n_patches):\n",
    "            for j in range(n_patches):\n",
    "                patch = image[\n",
    "                    :,\n",
    "                    i * patch_size : (i + 1) * patch_size,\n",
    "                    j * patch_size : (j + 1) * patch_size,\n",
    "                ]\n",
    "                patches[idx, i * n_patches + j] = patch.flatten()\n",
    "    return patches\n",
    "\n",
    "def get_positional_embeddings(sequence_length, d):\n",
    "    result = torch.zeros(sequence_length, d)\n",
    "    for i in range(sequence_length):\n",
    "        for j in range(0, d, 2):\n",
    "            result[i][j] = np.sin(i / (10000 ** (j / d)))\n",
    "            if j + 1 < d:\n",
    "                result[i][j + 1] = np.cos(i / (10000 ** (j / d)))\n",
    "    return result\n",
    "\n",
    "class MyMSA(nn.Module):\n",
    "    def __init__(self, d, n_heads=1):\n",
    "        super(MyMSA, self).__init__()\n",
    "        assert d % n_heads == 0\n",
    "        self.d_head = d // n_heads\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.q_mappings = nn.ModuleList([nn.Linear(self.d_head, self.d_head) for _ in range(n_heads)])\n",
    "        self.k_mappings = nn.ModuleList([nn.Linear(self.d_head, self.d_head) for _ in range(n_heads)])\n",
    "        self.v_mappings = nn.ModuleList([nn.Linear(self.d_head, self.d_head) for _ in range(n_heads)])\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, sequences):\n",
    "        output = []\n",
    "        for sequence in sequences:\n",
    "            heads_out = []\n",
    "            for h in range(self.n_heads):\n",
    "                start = h * self.d_head\n",
    "                end = (h + 1) * self.d_head\n",
    "                q = self.q_mappings[h](sequence[:, start:end])\n",
    "                k = self.k_mappings[h](sequence[:, start:end])\n",
    "                v = self.v_mappings[h](sequence[:, start:end])\n",
    "                attn = self.softmax(q @ k.T / (self.d_head ** 0.5))\n",
    "                heads_out.append(attn @ v)\n",
    "            output.append(torch.cat(heads_out, dim=1))\n",
    "        return torch.stack(output)\n",
    "\n",
    "class MyViTBlock(nn.Module):\n",
    "    def __init__(self, hidden_d, n_heads, mlp_ratio=2):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(hidden_d)\n",
    "        self.mhsa = MyMSA(hidden_d, n_heads)\n",
    "        self.norm2 = nn.LayerNorm(hidden_d)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_d, mlp_ratio * hidden_d),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_ratio * hidden_d, hidden_d),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.mhsa(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class MyViT(nn.Module):\n",
    "    def __init__(self, chw=(3, 32, 32), n_patches=4, n_blocks=1, hidden_d=64, n_heads=1, out_d=10):\n",
    "        super().__init__()\n",
    "        c, h, w = chw\n",
    "        assert h % n_patches == 0 and w % n_patches == 0\n",
    "        self.patch_size = (h // n_patches, w // n_patches)\n",
    "        self.input_d = c * self.patch_size[0] * self.patch_size[1]\n",
    "\n",
    "        self.linear_mapper = nn.Linear(self.input_d, hidden_d)\n",
    "        self.class_token = nn.Parameter(torch.rand(1, hidden_d))\n",
    "        self.register_buffer(\n",
    "            \"positional_embeddings\",\n",
    "            get_positional_embeddings(n_patches**2 + 1, hidden_d),\n",
    "            persistent=False\n",
    "        )\n",
    "\n",
    "        self.blocks = nn.ModuleList([MyViTBlock(hidden_d, n_heads) for _ in range(n_blocks)])\n",
    "        self.mlp_head = nn.Sequential(nn.Linear(hidden_d, out_d), nn.Softmax(dim=-1))\n",
    "\n",
    "    def forward(self, images):\n",
    "        n = images.shape[0]\n",
    "        patches = patchify(images, n_patches=4).to(self.positional_embeddings.device)\n",
    "        tokens = self.linear_mapper(patches)\n",
    "        tokens = torch.cat((self.class_token.expand(n, 1, -1), tokens), dim=1)\n",
    "        tokens += self.positional_embeddings.repeat(n, 1, 1)\n",
    "        for block in self.blocks:\n",
    "            tokens = block(tokens)\n",
    "        return self.mlp_head(tokens[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vwxFg_p5X357"
   },
   "source": [
    "Define Data Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ct_mrdCy5wLQ",
    "outputId": "c6cc63a1-946b-412a-96a8-f7325ddb09db"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [00:03<00:00, 43.1MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Define CIFAR-10 dataset transformations\n",
    "\n",
    "transform = Compose([\n",
    "    Resize((32, 32)),  # Ensure uniform size\n",
    "    RandomRotation(degrees=15),  # Apply random rotation between -15 to +15 degrees\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 train and test sets\n",
    "train_set = CIFAR10(\n",
    "    root=\"./../datasets\", train=True, download=True, transform=transform\n",
    ")\n",
    "test_set = CIFAR10(\n",
    "    root=\"./../datasets\", train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_set, shuffle=True, batch_size=128)\n",
    "test_loader = DataLoader(test_set, shuffle=False, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNHMuf_eYpRW"
   },
   "source": [
    "Define Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5jhsCxRs50AP",
    "outputId": "5bb314e1-de8a-404f-dde2-967aa1425ea0"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train Loss: 2.1875 | Val Loss: 2.1602\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50 | Train Loss: 2.1390 | Val Loss: 2.1280\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50 | Train Loss: 2.1204 | Val Loss: 2.1362\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50 | Train Loss: 2.1098 | Val Loss: 2.1051\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50 | Train Loss: 2.1089 | Val Loss: 2.1088\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50 | Train Loss: 2.1025 | Val Loss: 2.0973\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50 | Train Loss: 2.0977 | Val Loss: 2.0998\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50 | Train Loss: 2.0948 | Val Loss: 2.0942\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50 | Train Loss: 2.0923 | Val Loss: 2.0949\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50 | Train Loss: 2.0914 | Val Loss: 2.0939\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50 | Train Loss: 2.0802 | Val Loss: 2.0805\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50 | Train Loss: 2.0768 | Val Loss: 2.0840\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50 | Train Loss: 2.0770 | Val Loss: 2.0810\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50 | Train Loss: 2.0743 | Val Loss: 2.0834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50 | Train Loss: 2.0729 | Val Loss: 2.0857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50 | Train Loss: 2.0722 | Val Loss: 2.0785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50 | Train Loss: 2.0725 | Val Loss: 2.0762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50 | Train Loss: 2.0714 | Val Loss: 2.0837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50 | Train Loss: 2.0705 | Val Loss: 2.0805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20 Training:  33%|███▎      | 128/391 [01:20<02:57,  1.48it/s]"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Define the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MyViT((3, 32, 32), n_patches=4, n_blocks=2, hidden_d=8, n_heads=2, out_d=10).to(device)\n",
    "\n",
    "# Hyperparameters\n",
    "N_EPOCHS = 10\n",
    "LR = 0.005\n",
    "PATIENCE = 5  # Early stopping patience\n",
    "MIN_DELTA = 0.001  # Minimum improvement to reset early stopping\n",
    "STEP_SIZE = 10  # StepLR scheduler step size\n",
    "GAMMA = 0.5     # LR decay factor\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=LR)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n",
    "\n",
    "# Alternatively:\n",
    "# scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    # ---- Training ----\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1} Training\", leave=False):\n",
    "        x, y = batch\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_hat = model(x)\n",
    "        loss = criterion(y_hat, y)\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    # ---- Validation ----\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            x, y = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_hat = model(x)\n",
    "            loss = criterion(y_hat, y)\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(test_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{N_EPOCHS} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # ---- Scheduler Step ----\n",
    "    scheduler.step()  # For StepLR\n",
    "    # scheduler.step(val_loss)  # If using ReduceLROnPlateau\n",
    "\n",
    "    # ---- Early Stopping Check ----\n",
    "    if val_loss + MIN_DELTA < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), \"vit_best.pth\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L4FNu2C4Ysdz"
   },
   "source": [
    "Plot losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r8yVTPka7kTJ"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# Load the saved losses\n",
    "with open(\"loss_history.pkl\", \"rb\") as f:\n",
    "    train_losses, val_losses = pickle.load(f)\n",
    "\n",
    "# Plot the losses\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(train_losses, label=\"Training Loss\", marker='o')\n",
    "plt.plot(val_losses, label=\"Validation Loss\", marker='s')\n",
    "plt.title(\"Training vs Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "24k3YkBx9qFX"
   },
   "outputs": [],
   "source": [
    "# Save losses to file after training\n",
    "import pickle\n",
    "with open(\"loss_history.pkl\", \"wb\") as f:\n",
    "    pickle.dump((train_losses, val_losses), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7Ce7amCY2aB"
   },
   "source": [
    "Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k2ZpTkJz6Tl8"
   },
   "outputs": [],
   "source": [
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "correct, total = 0, 0\n",
    "test_loss = 0.0\n",
    "\n",
    "with torch.no_grad():  # No gradient calculation for evaluation\n",
    "    for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "        x, y = batch\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        # Get predictions and compute loss\n",
    "        y_hat = model(x)\n",
    "        loss = criterion(y_hat, y)\n",
    "\n",
    "        # Accumulate test loss\n",
    "        test_loss += loss.item()  # Use .item() to get the scalar value\n",
    "\n",
    "        # Compute accuracy\n",
    "        _, predicted = torch.max(y_hat, dim=1)  # Get the class predictions\n",
    "        correct += (predicted == y).sum().item()  # Sum the number of correct predictions\n",
    "        total += y.size(0)  # Total number of samples in this batch\n",
    "\n",
    "# Calculate final average test loss and accuracy\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "test_accuracy = (correct / total) * 100\n",
    "\n",
    "# Print results\n",
    "print(f\"Test Loss: {avg_test_loss:.2f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3aJmNKy4ZBU4"
   },
   "source": [
    "Finetuning/Transfer Learning Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4P6uB4WJ6lYH"
   },
   "outputs": [],
   "source": [
    "# Load the pre-trained model\n",
    "model.load_state_dict(torch.load(\"vit_cifar10.pth\"))\n",
    "model.to(device)\n",
    "\n",
    "# This is adapted for transfer learning. Unfreeze all layers if you want full fine-tuning\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Fine-tuning setup\n",
    "N_EPOCHS_FT = 5\n",
    "LR_FT = 0.001\n",
    "PATIENCE = 3\n",
    "MIN_DELTA = 0.001\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=LR_FT)\n",
    "scheduler = StepLR(optimizer, step_size=2, gamma=0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Early stopping setup\n",
    "best_val_loss = float(\"inf\")\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(N_EPOCHS_FT):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct_train, total_train = 0, 0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1} fine-tuning\", leave=False):\n",
    "        x, y = batch\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_hat = model(x)\n",
    "        loss = criterion(y_hat, y)\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, predicted = torch.max(y_hat, dim=1)\n",
    "        correct_train += (predicted == y).sum().item()\n",
    "        total_train += y.size(0)\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_accuracy = (correct_train / total_train) * 100\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_hat = model(x)\n",
    "            loss = criterion(y_hat, y)\n",
    "            val_loss += loss.item()\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{N_EPOCHS_FT} | Train Loss: {avg_train_loss:.4f} | \"\n",
    "          f\"Train Acc: {train_accuracy:.2f}% | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # Early stopping logic\n",
    "    if avg_val_loss + MIN_DELTA < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), \"vit_finetuned_best.pth\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g2RQFB-v7NGu"
   },
   "outputs": [],
   "source": [
    "# Load best model after fine-tuning\n",
    "model.load_state_dict(torch.load(\"vit_finetuned_best.pth\"))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
