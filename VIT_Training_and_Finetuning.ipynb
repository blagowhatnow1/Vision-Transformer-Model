{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eH85PsM4VfU7"
      },
      "outputs": [],
      "source": [
        "#https://medium.com/@brianpulfer/vision-transformers-from-scratch-pytorch-a-step-by-step-guide-96c3313c2e0c\n",
        "#Code for finetuning is added\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision.transforms import ToTensor, Compose, Normalize, Resize\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "torch.manual_seed(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Gi6taykVkhG",
        "outputId": "7abae132-6826-49d4-f73d-33005564357e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7d0929348d70>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define VIT architecture"
      ],
      "metadata": {
        "id": "FVObYKJveuZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def patchify(images, n_patches):\n",
        "    n, c, h, w = images.shape\n",
        "\n",
        "    assert h == w, \"Patchify method is implemented for square images only\"\n",
        "\n",
        "    patches = torch.zeros(n, n_patches**2, h * w * c // n_patches**2)\n",
        "    patch_size = h // n_patches\n",
        "\n",
        "    for idx, image in enumerate(images):\n",
        "        for i in range(n_patches):\n",
        "            for j in range(n_patches):\n",
        "                patch = image[\n",
        "                    :,\n",
        "                    i * patch_size : (i + 1) * patch_size,\n",
        "                    j * patch_size : (j + 1) * patch_size,\n",
        "                ]\n",
        "                patches[idx, i * n_patches + j] = patch.flatten()\n",
        "    return patches\n",
        "\n",
        "\n",
        "class MyMSA(nn.Module):\n",
        "    def __init__(self, d, n_heads=2):\n",
        "        super(MyMSA, self).__init__()\n",
        "        self.d = d\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "        assert d % n_heads == 0, f\"Can't divide dimension {d} into {n_heads} heads\"\n",
        "\n",
        "        d_head = int(d / n_heads)\n",
        "        self.q_mappings = nn.ModuleList(\n",
        "            [nn.Linear(d_head, d_head) for _ in range(self.n_heads)]\n",
        "        )\n",
        "        self.k_mappings = nn.ModuleList(\n",
        "            [nn.Linear(d_head, d_head) for _ in range(self.n_heads)]\n",
        "        )\n",
        "        self.v_mappings = nn.ModuleList(\n",
        "            [nn.Linear(d_head, d_head) for _ in range(self.n_heads)]\n",
        "        )\n",
        "        self.d_head = d_head\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, sequences):\n",
        "        # Sequences has shape (N, seq_length, token_dim)\n",
        "        # We go into shape    (N, seq_length, n_heads, token_dim / n_heads)\n",
        "        # And come back to    (N, seq_length, item_dim)  (through concatenation)\n",
        "        result = []\n",
        "        for sequence in sequences:\n",
        "            seq_result = []\n",
        "            for head in range(self.n_heads):\n",
        "                q_mapping = self.q_mappings[head]\n",
        "                k_mapping = self.k_mappings[head]\n",
        "                v_mapping = self.v_mappings[head]\n",
        "\n",
        "                seq = sequence[:, head * self.d_head : (head + 1) * self.d_head]\n",
        "                q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq)\n",
        "\n",
        "                attention = self.softmax(q @ k.T / (self.d_head**0.5))\n",
        "                seq_result.append(attention @ v)\n",
        "            result.append(torch.hstack(seq_result))\n",
        "        return torch.cat([torch.unsqueeze(r, dim=0) for r in result])\n",
        "\n",
        "\n",
        "class MyViTBlock(nn.Module):\n",
        "    def __init__(self, hidden_d, n_heads, mlp_ratio=4):\n",
        "        super(MyViTBlock, self).__init__()\n",
        "        self.hidden_d = hidden_d\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(hidden_d)\n",
        "        self.mhsa = MyMSA(hidden_d, n_heads)\n",
        "        self.norm2 = nn.LayerNorm(hidden_d)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_d, mlp_ratio * hidden_d),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(mlp_ratio * hidden_d, hidden_d),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x + self.mhsa(self.norm1(x))\n",
        "        out = out + self.mlp(self.norm2(out))\n",
        "        return out\n",
        "\n",
        "\n",
        "class MyViT(nn.Module):\n",
        "    def __init__(self, chw, n_patches=7, n_blocks=2, hidden_d=128, n_heads=8, out_d=10):\n",
        "        # Super constructor\n",
        "        super(MyViT, self).__init__()\n",
        "\n",
        "        # Attributes\n",
        "        self.chw = chw  # ( C , H , W )\n",
        "        self.n_patches = n_patches\n",
        "        self.n_blocks = n_blocks\n",
        "        self.n_heads = n_heads\n",
        "        self.hidden_d = hidden_d\n",
        "\n",
        "        # Input and patches sizes\n",
        "        assert (\n",
        "            chw[1] % n_patches == 0\n",
        "        ), \"Input shape not entirely divisible by number of patches\"\n",
        "        assert (\n",
        "            chw[2] % n_patches == 0\n",
        "        ), \"Input shape not entirely divisible by number of patches\"\n",
        "        self.patch_size = (chw[1] / n_patches, chw[2] / n_patches)\n",
        "\n",
        "        # 1) Linear mapper\n",
        "        self.input_d = int(chw[0] * self.patch_size[0] * self.patch_size[1])\n",
        "        self.linear_mapper = nn.Linear(self.input_d, self.hidden_d)\n",
        "\n",
        "        # 2) Learnable classification token\n",
        "        self.class_token = nn.Parameter(torch.rand(1, self.hidden_d))\n",
        "\n",
        "        # 3) Positional embedding\n",
        "        self.register_buffer(\n",
        "            \"positional_embeddings\",\n",
        "            get_positional_embeddings(n_patches**2 + 1, hidden_d),\n",
        "            persistent=False,\n",
        "        )\n",
        "\n",
        "        # 4) Transformer encoder blocks\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [MyViTBlock(hidden_d, n_heads) for _ in range(n_blocks)]\n",
        "        )\n",
        "\n",
        "        # 5) Classification MLPk\n",
        "        self.mlp = nn.Sequential(nn.Linear(self.hidden_d, out_d), nn.Softmax(dim=-1))\n",
        "\n",
        "    def forward(self, images):\n",
        "        # Dividing images into patches\n",
        "        n, c, h, w = images.shape\n",
        "        patches = patchify(images, self.n_patches).to(self.positional_embeddings.device)\n",
        "\n",
        "        # Running linear layer tokenization\n",
        "        # Map the vector corresponding to each patch to the hidden size dimension\n",
        "        tokens = self.linear_mapper(patches)\n",
        "\n",
        "        # Adding classification token to the tokens\n",
        "        tokens = torch.cat((self.class_token.expand(n, 1, -1), tokens), dim=1)\n",
        "\n",
        "        # Adding positional embedding\n",
        "        out = tokens + self.positional_embeddings.repeat(n, 1, 1)\n",
        "\n",
        "        # Transformer Blocks\n",
        "        for block in self.blocks:\n",
        "            out = block(out)\n",
        "\n",
        "        # Getting the classification token only\n",
        "        out = out[:, 0]\n",
        "\n",
        "        return self.mlp(out)  # Map to output dimension, output category distribution\n",
        "\n",
        "\n",
        "def get_positional_embeddings(sequence_length, d):\n",
        "    result = torch.ones(sequence_length, d)\n",
        "    for i in range(sequence_length):\n",
        "        for j in range(d):\n",
        "            result[i][j] = (\n",
        "                np.sin(i / (10000 ** (j / d)))\n",
        "                if j % 2 == 0\n",
        "                else np.cos(i / (10000 ** ((j - 1) / d)))\n",
        "            )\n",
        "    return result"
      ],
      "metadata": {
        "id": "jGirXpTuVsH9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess Data"
      ],
      "metadata": {
        "id": "Tt8sXTdwepBV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define CIFAR-10 dataset transformations\n",
        "transform = Compose([\n",
        "    Resize((32, 32)),  # Ensure the images are 32x32 (CIFAR-10 size)\n",
        "    ToTensor(),\n",
        "    Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalizing CIFAR-10 images\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 train and test sets\n",
        "train_set = CIFAR10(\n",
        "    root=\"./../datasets\", train=True, download=True, transform=transform\n",
        ")\n",
        "test_set = CIFAR10(\n",
        "    root=\"./../datasets\", train=False, download=True, transform=transform\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_set, shuffle=True, batch_size=128)\n",
        "test_loader = DataLoader(test_set, shuffle=False, batch_size=128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJDvJKF8VxZl",
        "outputId": "9ceae322-4bc4-4079-adbd-60c802962dc5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./../datasets/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:01<00:00, 90.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./../datasets/cifar-10-python.tar.gz to ./../datasets\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#For an arbitrary dataset stored under a directory \"MyData\"\n",
        "\n",
        "#from torchvision import datasets, transforms\n",
        "#from torch.utils.data import DataLoader\n",
        "#from torchvision.transforms import Compose, ToTensor, Normalize, Resize\n",
        "#from torchvision import datasets\n",
        "\n",
        "# Define dataset transformations\n",
        "#transform = Compose([\n",
        "#    Resize((32, 32)),  # Resize images to 32x32 (or adjust this based on your dataset)\n",
        "#    ToTensor(),\n",
        "#    Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalizing the images\n",
        "#])\n",
        "\n",
        "# Define paths to train and test folders (replace with your actual paths)\n",
        "#train_dir = \"MyData/train\"\n",
        "#test_dir = \"MyData/test\"\n",
        "\n",
        "# Load the train and test datasets\n",
        "#train_set = datasets.ImageFolder(root=train_dir, transform=transform)\n",
        "#test_set = datasets.ImageFolder(root=test_dir, transform=transform)\n",
        "\n",
        "# Create data loaders\n",
        "#train_loader = DataLoader(train_set, shuffle=True, batch_size=128)\n",
        "#test_loader = DataLoader(test_set, shuffle=False, batch_size=128)\n",
        "\n",
        "# Check the number of classes and the class labels\n",
        "#print(f\"Number of classes: {len(train_set.classes)}\")\n",
        "#print(f\"Class labels: {train_set.classes}\")"
      ],
      "metadata": {
        "id": "9-KQFEvWmapA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Loop"
      ],
      "metadata": {
        "id": "pyXlqy46ejqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = MyViT(\n",
        "    (3, 32, 32), n_patches=4, n_blocks=2, hidden_d=8, n_heads=2, out_d=10\n",
        ").to(device)\n",
        "\n",
        "# Hyperparameters for training\n",
        "N_EPOCHS = 5\n",
        "LR = 0.005  # Initial learning rate for training\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=LR)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(N_EPOCHS):\n",
        "    model.train()  # Set model to training mode\n",
        "    train_loss = 0.0\n",
        "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1} in training\", leave=False):\n",
        "        x, y = batch\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        y_hat = model(x)\n",
        "        loss = criterion(y_hat, y)\n",
        "\n",
        "        train_loss += loss.detach().cpu().item() / len(train_loader)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{N_EPOCHS} loss: {train_loss:.2f}\")\n",
        "\n",
        "# Save the trained model after initial training\n",
        "torch.save(model.state_dict(), \"vit_cifar10.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VokTDzGZWGDU",
        "outputId": "aba6a369-5c6e-4b48-e620-0506c1244f11"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 loss: 2.18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5 loss: 2.14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5 loss: 2.13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5 loss: 2.12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                      "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5 loss: 2.11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check model performance"
      ],
      "metadata": {
        "id": "0WprgDG5efS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "correct, total = 0, 0\n",
        "test_loss = 0.0\n",
        "\n",
        "with torch.no_grad():  # No gradient calculation for evaluation\n",
        "    for batch in tqdm(test_loader, desc=\"Testing\"):\n",
        "        x, y = batch\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        # Get predictions and compute loss\n",
        "        y_hat = model(x)\n",
        "        loss = criterion(y_hat, y)\n",
        "\n",
        "        # Accumulate test loss\n",
        "        test_loss += loss.item()  # Use .item() to get the scalar value\n",
        "\n",
        "        # Compute accuracy\n",
        "        _, predicted = torch.max(y_hat, dim=1)  # Get the class predictions\n",
        "        correct += (predicted == y).sum().item()  # Sum the number of correct predictions\n",
        "        total += y.size(0)  # Total number of samples in this batch\n",
        "\n",
        "# Calculate final average test loss and accuracy\n",
        "avg_test_loss = test_loss / len(test_loader)\n",
        "test_accuracy = (correct / total) * 100\n",
        "\n",
        "# Print results\n",
        "print(f\"Test Loss: {avg_test_loss:.2f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvpx1A61ZCJk",
        "outputId": "d7e97d96-3efd-4034-96cf-573baf1f18ce"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 79/79 [00:12<00:00,  6.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 2.08\n",
            "Test Accuracy: 37.03%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finetuning"
      ],
      "metadata": {
        "id": "s2JiBkcsebmx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Optionally added a learning rate scheduler\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "# Load the pre-trained model\n",
        "model.load_state_dict(torch.load(\"vit_cifar10.pth\"))\n",
        "model.to(device)\n",
        "\n",
        "# Unfreeze all layers if necessary (optional)\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "model.train()  # Set model to training mode\n",
        "\n",
        "# Fine-tuning setup\n",
        "N_EPOCHS_FT = 5  # Fine-tuning for 5 additional epochs\n",
        "LR_FT = 0.001  # Initial learning rate for fine-tuning\n",
        "optimizer = Adam(model.parameters(), lr=LR_FT)\n",
        "\n",
        "# Set up a learning rate scheduler\n",
        "scheduler = StepLR(optimizer, step_size=2, gamma=0.1)  # Decay learning rate every 2 epochs by 10%\n",
        "\n",
        "# Fine-tuning loop\n",
        "for epoch in range(N_EPOCHS_FT):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    correct_train, total_train = 0, 0\n",
        "\n",
        "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1} in fine-tuning\", leave=False):\n",
        "        x, y = batch\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        y_hat = model(x)\n",
        "        loss = criterion(y_hat, y)\n",
        "\n",
        "        train_loss += loss.item()  # Accumulate loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accuracy calculation\n",
        "        _, predicted = torch.max(y_hat, dim=1)\n",
        "        correct_train += (predicted == y).sum().item()\n",
        "        total_train += y.size(0)\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    train_accuracy = (correct_train / total_train) * 100\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{N_EPOCHS_FT} fine-tuning loss: {avg_train_loss:.2f}, \"\n",
        "          f\"Train Accuracy: {train_accuracy:.2f}%\")\n",
        "\n",
        "    # Step the learning rate scheduler\n",
        "    scheduler.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B60LOe7DWKBu",
        "outputId": "46688fb5-cd9d-4f03-d6c5-2432b7505b76"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-6fddb476c257>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"vit_cifar10.pth\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 fine-tuning loss: 2.09, Train Accuracy: 36.04%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5 fine-tuning loss: 2.09, Train Accuracy: 36.44%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5 fine-tuning loss: 2.08, Train Accuracy: 37.01%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5 fine-tuning loss: 2.08, Train Accuracy: 37.12%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                         "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5 fine-tuning loss: 2.08, Train Accuracy: 37.17%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the fine-tuned model\n",
        "model.eval()  # Set model to evaluation mode\n",
        "\n",
        "correct, total = 0, 0\n",
        "test_loss = 0.0\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_loader, desc=\"Testing\"):\n",
        "        x, y = batch\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        y_hat = model(x)\n",
        "        loss = criterion(y_hat, y)\n",
        "        test_loss += loss.detach().cpu().item() / len(test_loader)\n",
        "\n",
        "        correct += torch.sum(torch.argmax(y_hat, dim=1) == y).detach().cpu().item()\n",
        "        total += len(x)\n",
        "\n",
        "print(f\"Fine-Tuned Test loss: {test_loss:.2f}\")\n",
        "print(f\"Fine-Tuned Test accuracy: {correct / total * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ns5ccWLjXIFp",
        "outputId": "be57b644-327b-4b0a-bff4-d495f1ca906d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 79/79 [00:13<00:00,  5.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-Tuned Test loss: 2.08\n",
            "Fine-Tuned Test accuracy: 37.03%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fine-tuned model\n",
        "torch.save(model.state_dict(), \"vit_cifar10_finetuned.pth\")"
      ],
      "metadata": {
        "id": "IMSTZGHjXSyh"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}